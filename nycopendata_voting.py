# -*- coding: utf-8 -*-
"""NYCOpenData_Voting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hmZvXoW6Hb9_gLTu2z3wkjdO9vfvabE_
"""

# @title NYS Data Pipeline: Libraries
!pip install beautifulsoup4 requests pandas tqdm

# @title NYS Data Pipeline: Libraries 2
import csv
import logging
import pandas as pd
import re
import sys
from bs4 import BeautifulSoup
from requests import get
from tqdm import tqdm

# @title attempt
import csv
import logging
import pandas as pd
import sys
from requests import get
from tqdm import tqdm
from bs4 import BeautifulSoup

def download_parse():
    # Request user input for year
    year = input("Enter the election year (e.g., 2022): ")

    logging.info(f"Downloading current Election Results index page for year {year}.")
    url = "https://vote.nyc/page/election-results-summary"
    response = get(url)
    logging.info(
        "Got {:0,.0f} kb, parsing.".format(
            sys.getsizeof(response.text) / 1024))
    response.raise_for_status()

    # Parse the HTML content
    soup = BeautifulSoup(response.text, "lxml")

    # Extract available races/contests from the webpage
    races = set()  # To store unique races/contests
    for link in soup.find_all('a', href=True):
        href = link['href']
        if '.csv' in href and year in href:  # Ensure the CSV matches the year
            race = href.split('/')[-1].replace('.csv', '')  # Extract race info from URL
            races.add(race)

    if not races:
        logging.info(f"No races found for the year {year}.")
        return None  # Return None if no races found

    # Present the available races to the user
    print("\nAvailable races/contests:")
    for idx, race in enumerate(races, start=1):
        print(f"{idx}. {race}")

    # Request user to select a race
    race_choice = int(input(f"\nSelect a race by number (1-{len(races)}): ")) - 1
    selected_race = list(races)[race_choice]

    logging.info(f"Selected race: {selected_race}")

    # Filter the CSVs based on the selected year and race
    urls = []
    for link in soup.find_all('a', href=True):
        href = link['href']
        if '.csv' in href and year in href and selected_race in href:
            if href[0] == '/':
                urls.append(f"https://vote.nyc{href}")
            else:
                urls.append(href)

    # If no matching CSVs are found, inform the user and exit
    if not urls:
        logging.info(f"No CSVs found for the year {year} and race {selected_race}.")
        return None

    # Download the selected CSV and parse its data
    selected_url = urls[0]  # Assuming we take the first URL if multiple are available
    logging.info(f"Downloading file from: {selected_url}")
    response = get(selected_url)
    response.raise_for_status()

    # Read the CSV data
    lines = response.text.splitlines()
    print(f"First few lines of {selected_url}:")
    print(lines[:10])  # Print the first 10 lines of the CSV content

    # Clean up the CSV data: skip the first few rows (metadata) and find relevant rows
    clean_lines = []
    data_started = False
    for line in lines:
        # Look for a line that indicates the start of the relevant data
        if 'Total Ballots' in line:  # Assuming this marks the start of election results
            data_started = True
        if data_started:
            clean_lines.append(line)

    # Use a CSV reader to read the clean data
    reader = csv.reader(clean_lines)

    # Dynamically assign columns based on the actual header in the CSV
    header = next(reader)  # Get the header row
    print("Header:", header)

    # Filter out irrelevant rows and adjust the header if needed
    data = []
    for row in reader:
        if len(row) > 1:  # Check if the row has sufficient data
            data.append(row)

    # Create a DataFrame from the processed data
    df = pd.DataFrame(data, columns=header)
    print("DataFrame columns:", df.columns)

    # Clean the DataFrame: remove extra spaces or convert values as needed
    df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)  # Clean up whitespace in text columns

    return df

def run():
    df = download_parse()

    if df is None:  # Exit if no data was found
        print("No data found for the provided year and race.")
        return

    # Process data as needed (e.g., saving to file, further analysis, etc.)
    print("Processed Election Data:")
    print(df.head())

run()

# @title NYS Data Pipeline: helpers
def get_date(row):
    m = re.match(r'(.*) - (\d{2})/(\d{2})/(\d{4})', row['Event'])
    if m:
        return "{}{}{}".format(m.group(4), m.group(2), m.group(3))
    else:
        return None


def get_election_type(row):
    m = re.match(r'(.*) Election \d\d\d\d - (\d{2})/(\d{2})/(\d{4})', row['Event'])
    if m:
        return m.group(1).lower()
    else:
        return "special"


def get_candidate(row):
    if (row['election_type'] == 'primary') or (
            row['UnitName'] in SPECIAL_TYPES):
        return row['UnitName']
    else:
        m = re.match(r"(.*)? \((.*)\)", row['UnitName'])
        if m:
            return m.group(1)
        else:
            return None


def get_party(row):
    if row['election_type'] == 'primary':
        return row['PartyIndependentBody']
    if row['UnitName'] in SPECIAL_TYPES:
        return None
    else:
        m = re.match(r"(.*)? \((.*)\)", row['UnitName'])
        if m:
            return m.group(2)
        else:
            return None


def get_transformed_df(df, filename):
    output = df.loc[df['filename'] == filename,
                    ['County',
                     'precinct',
                     'OfficePositionTitle',
                     'DistrictKey',
                     'candidate',
                     'party',
                     'Tally']] .rename(columns={'County': 'county',
                                                'OfficePositionTitle': 'office',
                                                'DistrictKey': 'district',
                                                'Tally': 'votes'})
    return output.drop_duplicates()

# @title NYS Data Pipeline: run processing
def run():
    df = download_parse()
    logging.info("Converting dates.")
    df['date'] = df.apply(get_date, axis=1)
    logging.info("Converting election types.")
    df['election_type'] = df.apply(get_election_type, axis=1)
    logging.info("Generating filenames.")
    df['filename'] = df.apply(lambda row: "{}__ny__{}__{}__precinct.csv".format(
        row['date'], row['election_type'], row['County'].lower()), axis=1)
    logging.info("Converting precinct codes.")
    df['precinct'] = df.apply(lambda row: "{:03.0f}/{:02.0f}".format(
        int(row['ED']), int(row['AD'])), axis=1)
    logging.info("Converting candidate names.")
    df['candidate'] = df.apply(lambda row: get_candidate(row), axis=1)
    logging.info("Converting party names.")
    df['party'] = df.apply(lambda row: get_party(row), axis=1)
    files = df.loc[:, 'filename'].drop_duplicates()
    logging.info("Writing files.")
    for filename in tqdm(files):
        output = get_transformed_df(df, filename)
        output.to_csv(f"/content/{filename}", index=False)  # Save to Colab's environment

run()

# @title NYS Data Pipeline: download generated files
from google.colab import files
import os

# After running the code, you can download the output files
for filename in os.listdir('/content'):
    if filename.endswith(".csv"):
        files.download(f"/content/{filename}")

# Commented out IPython magic to ensure Python compatibility.
# %load_ext rpy2.ipython

# Commented out IPython magic to ensure Python compatibility.
# %%R
# install.packages("tidyverse")
# install.packages("janitor")
# library(tidyverse)
# library(janitor)
# 
# 
# results_url_2016 = "https://vote.nyc/sites/default/files/pdf/election_results/2016/20161108General%20Election/00000100000Citywide%20President%20Vice%20President%20Citywide%20EDLevel.csv"
# 
# results_url_2020 = "https://vote.nyc/sites/default/files/pdf/election_results/2020/20201103General%20Election/00000100000Citywide%20President%20Vice%20President%20Citywide%20EDLevel.csv"
# 
# results_url_2024 = "https://vote.nyc/sites/default/files/pdf/election_results/2024/20241105General%20Election/00000100000Citywide%20President%20Vice%20President%20Citywide%20EDLevel.csv"
# 
# results_file_2016 = "/content/results_2016.csv"
# results_file_2020 = "/content/results_2020.csv"
# results_file_2024 = "/content/results_2024.csv"
# 
# download.file(results_url_2016, results_file_2016)
# download.file(results_url_2020, results_file_2020)
# download.file(results_url_2024, results_file_2024)
# 
# election_results_2016 = read_csv(results_file_2016, guess_max = 10e3) %>%
#   janitor::clean_names() %>%
#   mutate(
#     elect_dist = as.numeric(paste0(ad, ed)),
#     candidate = str_replace(unit_name, " \\(.+?\\)$", "")
#   ) %>%
#   group_by(elect_dist) %>%
#   summarize(
#     republican = sum(tally * grepl("Trump", candidate)),
#     democratic = sum(tally * grepl("Clinton", candidate)),
#     green = sum(tally * grepl("Stein", candidate)),
#     libertarian = sum(tally * grepl("Johnson", candidate)),
#     .groups = "drop"
#   ) %>%
#   mutate(year = 2016)
# 
# raw_certified_2020_results = read_csv(results_file_2020, col_names = FALSE, guess_max = 100e3)
# col_names_2020 = janitor::make_clean_names(raw_certified_2020_results[1, 1:11])
# 
# election_results_2020 = raw_certified_2020_results[, 12:22] %>%
#   set_names(col_names_2020) %>%
#   mutate(
#     elect_dist = as.numeric(paste0(ad, ed)),
#     candidate = str_replace(unit_name, " \\(.+?\\)$", "")
#   ) %>%
#   group_by(elect_dist) %>%
#   summarize(
#     republican = sum(tally * grepl("Trump", candidate)),
#     democratic = sum(tally * grepl("Biden", candidate)),
#     green = sum(tally * grepl("Hawkins", candidate)),
#     libertarian = sum(tally * grepl("Jorgensen", candidate)),
#     .groups = "drop"
#   ) %>%
#   mutate(year = 2020)
# 
# raw_certified_2024_results = read_csv(results_file_2024, col_names = FALSE, guess_max = 100e3)
# col_names_2024 = janitor::make_clean_names(raw_certified_2024_results[1, 1:11])
# 
# election_results_2024 = raw_certified_2024_results[, 12:22] %>%
#   set_names(col_names_2024) %>%
#   mutate(
#     elect_dist = as.numeric(paste0(ad, ed)),
#     candidate = str_replace(unit_name, " \\(.+?\\)$", "")
#   ) %>%
#   group_by(elect_dist) %>%
#   summarize(
#     republican = sum(tally * grepl("Trump", candidate)),
#     democratic = sum(tally * grepl("Harris", candidate)),
#     .groups = "drop"
#   ) %>%
#   mutate(year = 2024)
# 
# bind_rows(
#   election_results_2016,
#   election_results_2020,
#   election_results_2024
# ) %>%
#   mutate(
#     green = replace_na(green, 0),
#     libertarian = replace_na(libertarian, 0)
#   ) %>%
#   write_csv("nyc_election_results_by_district.csv")
#

# @title Create Pivot Table
import pandas as pd

# Load the data from the specified path, without headers
df = pd.read_csv('/content/results_2024.csv', header=None)

# Pad 'L' (index 11) and 'M' (index 12) columns to make sure they always have the correct number of digits
df[11] = df[11].astype(str).str.zfill(2)  # Padding L (12th column) to 2 digits
df[12] = df[12].astype(str).str.zfill(3)  # Padding M (13th column) to 3 digits

# Concatenate 'L' and 'M' columns to form a 5-digit string
df['Concatenated'] = df[11] + df[12]

# The relevant columns for pivoting should be the vote categories (column U) and tally (column V)
vote_categories = df[20]  # Column U contains the vote categories
tallies = df[21]  # Column V contains the tallies

# Now, let's create a new DataFrame that contains the concatenated value and the respective vote categories with tallies
pivot_data = pd.DataFrame({
    'Concatenated': df['Concatenated'],
    'Vote Category': vote_categories,
    'Tally': tallies
})

# Clean the Tally column by removing commas and converting it to integers
pivot_data['Tally'] = pivot_data['Tally'].replace({',': ''}, regex=True)  # Remove commas
pivot_data['Tally'] = pd.to_numeric(pivot_data['Tally'], errors='coerce')  # Convert to numeric, coercing errors to NaN

# Now, let's pivot the data
pivot_table = pivot_data.pivot_table(index='Concatenated', columns='Vote Category', values='Tally', aggfunc='sum', fill_value=0)

# Convert all values in the pivot table to integers (as they are now numeric)
pivot_table = pivot_table.astype(int)

# Save the pivot table to a CSV file in /content
pivot_table.to_csv('/content/election_results_pivot.csv', index=True, header=True)

# Confirming that the file was saved
print("Pivot table has been saved to /content/election_results_pivot.csv")

# @title Summarize Election Results

pivot_table = pd.read_csv('/content/election_results_pivot.csv')

# Create election_summary table
election_summary = pd.DataFrame()
election_summary['ed'] = pivot_table['Concatenated']

# Compute new columns
election_summary['TrumpVance'] = (
    pivot_table['Donald J. Trump / JD Vance (Conservative)'] +
    pivot_table['Donald J. Trump / JD Vance (Republican)']
)

election_summary['HarrisWalz'] = (
    pivot_table['Kamala D. Harris / Tim Walz (Democratic)'] +
    pivot_table['Kamala D. Harris / Tim Walz (Working Families)']
)

election_summary['HarrisMinTrump'] = (
    election_summary['HarrisWalz'] - election_summary['TrumpVance']
)

# Fill any remaining NaN values with 0
election_summary.fillna(0, inplace=True)

# Save summary table
election_summary.to_csv('/content/election_summary.csv', index=True, header=True)

# Confirm file save
print("Election summary saved to /content/election_summary.csv")
print(election_summary.shape[0])
print(election_summary.head())

# @title Fetch Election Districts

# # Install required packages
# !pip install geopandas arcgis requests tqdm

# import geopandas as gpd
# import requests
# from tqdm import tqdm

# Define the ArcGIS Feature Service URL
url = "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Election_Districts/FeatureServer/0/query"

# Parameters for querying the data in chunks (pagination)
params = {
    "where": "1=1",  # Fetch all records
    "outFields": "*",  # Retrieve all fields
    "outSR": "4326",  # Output Spatial Reference (WGS 84)
    "f": "geojson",  # Output format as GeoJSON
    "resultOffset": 0,  # Start offset for pagination
    "resultRecordCount": 1000  # Number of records per request (max usually 1000)
}

all_features = []

# Fetch all records using pagination
print("Fetching election district data...")
while True:
    response = requests.get(url, params=params)
    if response.status_code != 200:
        print(f"Failed to fetch data. Status Code: {response.status_code}")
        break

    data = response.json()

    if "features" not in data or not data["features"]:
        break  # Exit loop when no more records are returned

    all_features.extend(data["features"])
    params["resultOffset"] += params["resultRecordCount"]  # Move to next set of records

    print(f"Retrieved {len(all_features)} records...")

# Convert to a GeoDataFrame
if all_features:
    gdf = gpd.GeoDataFrame.from_features(all_features, crs="EPSG:4326")

    # Save to a GeoPackage file
    gdf.to_file("nyed.gpkg", driver="GPKG")
    print(f"Saved full dataset to nyed.gpkg successfully! Total records: {len(gdf)}")
else:
    print("No data retrieved.")

# @title Join Election Results to Districts

import geopandas as gpd

# Reset the index to ensure 'Concatenated' is only a column and not an index level
election_summary = election_summary.reset_index(drop=True)

# Check if 'Concatenated' exists in election_summary
print("Columns in election_summary:", election_summary.columns)

# Load the election districts dataset
nyed = gpd.read_file('/content/nyed.gpkg')

# Check if 'ElectDist' exists in nyed before merging
print("Columns in nyed:", nyed.columns)

if 'ElectDist' not in nyed.columns:
    raise KeyError("The column 'ElectDist' does not exist in nyed. Check the column names!")

# Perform the join
nyed = nyed.merge(election_summary, left_on='ElectDist', right_on='ed', how='left')

# Save the result
output_path = "/content/nyed_votes.gpkg"
nyed.to_file(output_path, driver="GPKG")

# Print the first few rows to verify the merge
print(nyed.head())

# @title Map election results
import folium
import numpy as np
import branca.colormap as cm

# Ensure the data is projected correctly
nyed = nyed.to_crs(epsg=4326)

# Get the dataset centroid for proper centering
centroid = nyed.geometry.centroid.unary_union.centroid

# Define a symmetric color scale around zero for HarrisMinTrump
max_abs_value = max(abs(nyed["HarrisMinTrump"].min()), nyed["HarrisMinTrump"].max())
colormap = cm.LinearColormap(
    colors=["red", "white", "blue"],
    vmin=-max_abs_value,
    vmax=max_abs_value,
    caption="Harris Minus Trump Votes"
)

# Create a Folium map
m = folium.Map(
    location=[centroid.y, centroid.x],  # Ensure proper centering
    zoom_start=10,
    tiles="cartodb positron"
)

# Function to style each district based on HarrisMinTrump value
def style_function(feature):
    value = feature["properties"]["HarrisMinTrump"]
    return {
        "fillColor": colormap(value),
        "color": "black",
        "weight": 1,
        "fillOpacity": 0.7,
    }

# Add the GeoJSON layer using the election district boundaries
folium.GeoJson(
    nyed,
    name="Election Districts",
    tooltip=folium.GeoJsonTooltip(fields=["ElectDist", "HarrisMinTrump"],
                                  aliases=["Election District:", "Harris - Trump Votes:"]),
    style_function=style_function
).add_to(m)

# Add the color legend
colormap.add_to(m)

# Display the map
from IPython.display import display
display(m)

# @title Fetch Poll Sites

# URL of the ArcGIS Feature Server layer
url = "https://services6.arcgis.com/EbVsqZ18sv1kVJ3k/ArcGIS/rest/services/NYS_Elections_Districts_and_Polling_Locations/FeatureServer/3/query?where=1%3D1&outFields=*&outSR=4326&f=geojson"

# Load the data into a GeoDataFrame
pollsites = gpd.read_file(url)

# Save to a GeoPackage file in the content directory
output_path = "/content/pollsites.gpkg"
pollsites.to_file(output_path, driver="GPKG")

# Confirm the file has been saved
output_path

# @title Fetch subway entrances and exits

import requests
import json

# API endpoint
base_url = "https://data.ny.gov/resource/i9wp-a4ja.geojson"
limit = 1000  # Maximum records per request
offset = 0  # Start at the first record

# Initialize GeoJSON structure
geojson_data = {
    "type": "FeatureCollection",
    "features": []
}

while True:
    # Construct the API request URL with pagination
    url = f"{base_url}?$limit={limit}&$offset={offset}"

    # Fetch data from the API
    response = requests.get(url)

    if response.status_code != 200:
        print(f"Error: Unable to fetch data (Status Code: {response.status_code})")
        break

    data = response.json()

    # If no more data is returned, stop the loop
    if "features" not in data or not data["features"]:
        break

    # Append features to the GeoJSON structure
    geojson_data["features"].extend(data["features"])

    # Increment offset for pagination
    offset += limit

# Save the GeoJSON file
geojson_output_path = "/content/subwayentranceexit.geojson"
with open(geojson_output_path, "w") as f:
    json.dump(geojson_data, f, indent=4)

# Confirm file save
geojson_output_path

# @title Fetch Residential Buildings

import geopandas as gpd
import urllib.parse

# Base URL for the MAPPLUTO dataset
base_url = "https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/MAPPLUTO/FeatureServer/0/query"

# Properly encode the SQL query
where_clause = "LandUse IN ('01','02','03','04')"
encoded_where = urllib.parse.quote(where_clause)

# Construct the full query URL
query_params = f"?where={encoded_where}&outFields=*&outSR=4326&f=geojson"
mappluto_url = base_url + query_params

# Load the data into a GeoDataFrame
mappluto_gdf = gpd.read_file(mappluto_url)

# Save to a local GeoJSON file
output_path = "/content/mappluto_filtered.geojson"
mappluto_gdf.to_file(output_path, driver="GeoJSON")

# Display dataset information
print(f"Filtered data saved to {output_path}")
print("Columns in dataset:", mappluto_gdf.columns)

# Show the first few rows
mappluto_gdf.head()

# @title Set CRS

subway_entrances = gpd.read_file('/content/subwayentranceexit.geojson')
election_districts = gpd.read_file('/content/nyed_votes.gpkg')

poll_sites = pollsites.to_crs(epsg=4326)
subway_entrances = subway_entrances.to_crs(epsg=4326)
election_districts = election_districts.to_crs(epsg=4326)

# @title Calculate Distance Between Poll Sites and Subway Entrances
from scipy.spatial import cKDTree
import numpy as np

# Extract poll site and subway coordinates
poll_coords = np.array(list(zip(poll_sites.geometry.x, poll_sites.geometry.y)))
subway_coords = np.array(list(zip(subway_entrances.geometry.x, subway_entrances.geometry.y)))

# Use KDTree for fast nearest-neighbor search
subway_tree = cKDTree(subway_coords)
distances, indices = subway_tree.query(poll_coords)

# Add distance to poll_sites DataFrame
poll_sites["nearest_subway_dist"] = distances  # Distance in degrees (~111km per degree)
poll_sites["nearest_subway_id"] = indices  # Index of nearest subway entrance

# Convert degrees to meters (~111,000 meters per degree)
poll_sites["nearest_subway_dist_m"] = poll_sites["nearest_subway_dist"] * 111000

# @title Calculate Distance From Housing Units to Poll Sites
from scipy.spatial import cKDTree
import numpy as np
import geopandas as gpd


# Convert to a projected CRS (New York State Plane, EPSG:2263 for NYC)
mappluto_gdf = mappluto_gdf.to_crs(epsg=2263)
poll_sites = poll_sites.to_crs(epsg=2263)

# Compute centroids in projected CRS for accurate distance calculations
mappluto_gdf["centroid"] = mappluto_gdf.geometry.centroid

# Extract centroid coordinates for buildings
building_coords = np.array(list(zip(mappluto_gdf.centroid.x, mappluto_gdf.centroid.y)))

# Extract poll site coordinates
poll_coords = np.array(list(zip(poll_sites.geometry.x, poll_sites.geometry.y)))

# Use KDTree for fast nearest-neighbor search
poll_tree = cKDTree(poll_coords)
distances, indices = poll_tree.query(building_coords)

# Add distance and nearest poll site ID to mappluto_gdf DataFrame
mappluto_gdf["nearest_poll_dist_m"] = distances  # Distance in meters (accurate in projected CRS)
mappluto_gdf["nearest_poll_id"] = indices  # Index of nearest poll site

# Convert back to WGS 84 for export
mappluto_gdf = mappluto_gdf.to_crs(epsg=4326)

# Drop extra geometry column (keeping only the original building geometry)
mappluto_gdf = mappluto_gdf.drop(columns=["centroid"])

# Save updated dataset
output_path = "/content/mappluto_with_poll_distances.geojson"
mappluto_gdf.to_file(output_path, driver="GeoJSON")

# Confirm completion
print(f"Updated MAPPLUTO dataset saved to {output_path}")

#  @title Categorize Accessibility

def accessibility_category(distance):
    if distance <= 200:  # 200m is ~2 blocks
        return "Highly Accessible"
    elif distance <= 500:  # 500m is ~5 blocks
        return "Moderately Accessible"
    elif distance <= 1000:  # 1km (~0.6 miles)
        return "Low Accessibility"
    else:
        return "Not Easily Accessible"

poll_sites["accessibility_category"] = poll_sites["nearest_subway_dist_m"].apply(accessibility_category)

# @title Aggregate metro accessibility by election district

import geopandas as gpd
from scipy.spatial import cKDTree
import numpy as np
import pandas as pd

# Rename election district column to match
election_districts.rename(columns={"ElectDist": "district_id"}, inplace=True)

# Aggregate mean subway accessibility by district
district_accessibility = poll_sites_with_districts.groupby("district_id")["nearest_subway_dist_m"].mean().reset_index()

# Merge aggregated accessibility with election districts
election_districts = election_districts.merge(district_accessibility, on="district_id", how="left")

# Identify districts with missing subway distances
missing_districts = election_districts["nearest_subway_dist_m"].isna().sum()
print(f"Missing districts without poll site accessibility: {missing_districts}")

# If a district has no poll site, assign the nearest poll site's accessibility
if missing_districts > 0:
    # Extract centroids of election districts for spatial lookup
    district_centroids = election_districts.geometry.centroid
    district_coords = np.array(list(zip(district_centroids.x, district_centroids.y)))

    # Extract poll site locations
    poll_coords = np.array(list(zip(poll_sites_with_districts.geometry.x, poll_sites_with_districts.geometry.y)))
    poll_distances = poll_sites_with_districts["nearest_subway_dist_m"].values

    # Build a KDTree for nearest-neighbor search
    poll_tree = cKDTree(poll_coords)

    # Find the nearest poll site for each district centroid
    distances, indices = poll_tree.query(district_coords)

    # Assign the nearest poll site's accessibility to districts without one
    election_districts["nearest_subway_dist_m"].fillna(pd.Series(poll_distances[indices]), inplace=True)

# Save updated dataset
election_districts.to_file('/content/districts_w_dist.gpkg', driver="GPKG")

# Confirm completion
print("Updated election districts with nearest poll site accessibility saved!")

# @title Aggregate house/poll site accessibility by election district

# Ensure there is no 'index_right' column before joining
if "index_right" in mappluto_gdf.columns:
    mappluto_gdf = mappluto_gdf.drop(columns=["index_right"])

if "index_right" in election_districts.columns:
    election_districts = election_districts.drop(columns=["index_right"])

# Ensure both datasets have the same CRS for spatial join
mappluto_gdf = mappluto_gdf.to_crs(election_districts.crs)

# Perform spatial join to assign each building to an election district
mappluto_gdf = mappluto_gdf.sjoin(election_districts[["district_id", "geometry"]], how="left", predicate="within")

# Check if district assignment worked
print("Updated MapPLUTO with Election Districts:", mappluto_gdf[["district_id", "nearest_poll_dist_m"]].head())

# Aggregate mean house-to-poll-site accessibility by district
district_house_accessibility = mappluto_gdf.groupby("district_id")["nearest_poll_dist_m"].mean().reset_index()

# Merge aggregated accessibility with election districts
election_districts = election_districts.merge(district_house_accessibility, on="district_id", how="left")

# Save updated dataset
output_path = "/content/districts_house_poll_accessibility.gpkg"
election_districts.to_file(output_path, driver="GPKG")

print(f"✅ Updated election districts with house-to-poll accessibility saved to {output_path}")

# @title Identify Election Districts with Poor Accessibility


# Sort election districts by highest average distance to subway
least_accessible = election_districts.sort_values("nearest_subway_dist_m", ascending=False)

# Display top 10 least accessible districts
print(least_accessible[['district_id', 'nearest_subway_dist_m']].head(10))

# @title Map accessibility by election district

import folium
import branca.colormap as cm

# Create a folium map centered on NYC
m = folium.Map(location=[40.7128, -74.0060], zoom_start=11, tiles="cartodb positron")

# Define a color scale
colormap = cm.LinearColormap(
    colors=["green", "yellow", "red"],
    vmin=election_districts["nearest_subway_dist_m"].min(),
    vmax=election_districts["nearest_subway_dist_m"].max(),
    caption="Poll Site Distance to Nearest Subway (meters)"
)

# Add election districts as a choropleth layer
folium.GeoJson(
    election_districts,
    name="Poll Site Accessibility",
    tooltip=folium.GeoJsonTooltip(fields=["district_id", "nearest_subway_dist_m"], aliases=["District:", "Avg Distance (m):"]),
    style_function=lambda feature: {
        "fillColor": colormap(feature["properties"]["nearest_subway_dist_m"]) if feature["properties"]["nearest_subway_dist_m"] is not None else "gray",
        "color": "black",
        "weight": 1,
        "fillOpacity": 0.7,
    }
).add_to(m)

# Add legend
colormap.add_to(m)

# Display the map
m

# @title Language Minorities by Bourough
import pandas as pd
import requests
from io import StringIO

# API endpoint
base_url = "https://data.cityofnewyork.us/resource/ajin-gkbp.csv"
limit = 1000  # Max rows per request
offset = 0  # Start at first record

# List to store all data
all_data = []

while True:
    # Construct request URL with pagination
    url = f"{base_url}?$limit={limit}&$offset={offset}"

    # Fetch data from API
    response = requests.get(url)

    if response.status_code != 200:
        print(f"Error fetching data (Status Code: {response.status_code})")
        break

    # Convert response to DataFrame
    data = pd.read_csv(StringIO(response.text))

    # If no data is returned, stop the loop
    if data.empty:
        break

    # Append the data
    all_data.append(data)

    # Increment offset for pagination
    offset += limit

# Combine all data into a single DataFrame
if all_data:
    full_data = pd.concat(all_data, ignore_index=True)
    print("Columns in dataset:", full_data.columns)
else:
    print("No data retrieved from API.")

full_data.to_csv('/content/language_data.csv', index=False)

# @title Language Minorities Census